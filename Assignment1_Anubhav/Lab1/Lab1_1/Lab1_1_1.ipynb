{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "313d9970",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22602f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kumar\\AppData\\Local\\Temp\\ipykernel_25604\\3095012978.py:10: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  train_df = pd.read_csv(\"FCNN_Lab1_dataset/adult.data\",names = columns, sep = ', *')\n",
      "C:\\Users\\kumar\\AppData\\Local\\Temp\\ipykernel_25604\\3095012978.py:11: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  test_df = pd.read_csv('FCNN_Lab1_dataset/adult.test', names=columns, sep=', *',skiprows=1)\n"
     ]
    }
   ],
   "source": [
    "# loading the dataset\n",
    "\n",
    "# since no columns are there in the dataset\n",
    "columns = [\n",
    "    \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\",\n",
    "    \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\",\n",
    "    \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\", \"income\"\n",
    "]\n",
    "\n",
    "train_df = pd.read_csv(\"FCNN_Lab1_dataset/adult.data\",names = columns, sep = ', *')\n",
    "test_df = pd.read_csv('FCNN_Lab1_dataset/adult.test', names=columns, sep=', *',skiprows=1)\n",
    "\n",
    "train_df['income'] = train_df['income'].str.replace('.', '', regex=False).str.strip()\n",
    "test_df['income'] = test_df['income'].str.replace('.', '', regex=False).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b7c5a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dealing with NA values, by filling with Mode\n",
    "\n",
    "columns_with_na = train_df.columns[train_df.isnull().any()].tolist()\n",
    "\n",
    "for col in columns_with_na:\n",
    "    mode_value = train_df[col].mode()[0]\n",
    "    train_df[col] = train_df[col].fillna(mode_value)\n",
    "    test_df[col] = test_df[col].fillna(mode_value)\n",
    "\n",
    "#now separating the output from the input features, in both train and test sets\n",
    "#train sets\n",
    "x_t_r = train_df.drop(\"income\",axis=1)\n",
    "y_t_r = train_df[\"income\"].map({\"<=50K\":0, \">50K\":1})\n",
    "\n",
    "#test sets\n",
    "x_te_r = test_df.drop(\"income\",axis =1)\n",
    "y_te_r = test_df[\"income\"].map({\"<=50K\":0, \">50K\":1})\n",
    "\n",
    "#one hot encoding\n",
    "x_train = pd.get_dummies(x_t_r)\n",
    "x_test = pd.get_dummies(x_te_r)\n",
    "\n",
    "#handling the case in which if the test set doesnt have a particular feature value the one hot encoded vector wont be aligned with the training data in whihch the particular extra feature did appear.\n",
    "\n",
    "x_train, x_test = x_train.align(x_test,join='left', axis=1, fill_value=0) # fill in the left values as 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81cb5d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32561, 108)\n"
     ]
    }
   ],
   "source": [
    "#converting to np\n",
    "\n",
    "x_train = x_train.values.astype(np.float32)\n",
    "y_train = y_t_r.values.astype(np.float32).reshape(-1,1) # reshaping the 1-D vector to a 2-D vector so to calculate the loss yi for each input xi\n",
    "\n",
    "x_test = x_test.values.astype(np.float32)\n",
    "y_test = y_te_r.values.astype(np.float32).reshape(-1,1)\n",
    "print(x_train.shape) # gives the number of input neurons required, here 108 are required\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6513348",
   "metadata": {},
   "source": [
    "## **FCNN implementation**\n",
    "\n",
    "1. total neurons in input layer = 108 \n",
    "2. total neurons in 1st hidden layer = 65, total neurons in second hidden layer = 33\n",
    "3. total neurons in output layer = 1\n",
    "4. total weights in 1st layer = 108*65 = 7020 + 65 biases = 7085 in layer 1->2\n",
    "5. total weights and biases in 2nd layer = 65*33 + 33 biases = 2178 in layer 2->3\n",
    "6. total weights and biases in layer 3-> output = 34\n",
    "7. total parameters to tune = 9297"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34a296cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(x,theta): # relu for hidden and sigmoid for output\n",
    "    A1 = np.dot(x,theta[\"w1\"]) + theta[\"b1\"]\n",
    "    H1 = np.maximum(0,A1) #ReLU activation\n",
    "\n",
    "    A2 = np.dot(H1,theta[\"w2\"]) + theta[\"b2\"]\n",
    "    H2 = np.maximum(0,A2)\n",
    "\n",
    "    A3 = np.dot(H2,theta[\"w3\"]) + theta[\"b3\"]\n",
    "    A3_clipped = np.clip(A3, -500, 500)\n",
    "    H3 = 1 / (1 + np.exp(-A3_clipped))\n",
    "\n",
    "    cache = {\"A1\":A1, \"H1\":H1, \"A2\":A2, \"H2\":H2, \"A3\":A3, \"H3\":H3}\n",
    "    return cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03c59fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(x,y,cache,theta):\n",
    "    # compute the derivative of loss fn wrt output unit (y_exp (is just H3) then aL), the required loss function is the cross-entropy loss\n",
    "    dA3 = cache[\"H3\"] - y\n",
    "    # now we compute the grad wrt w3 and b3 (before out layer)\n",
    "    m = x.shape[0]\n",
    "    dw3 = (1/m)*(np.dot(cache[\"H2\"].T,dA3))\n",
    "    db3 = (1/m)*np.sum(dA3, axis=0, keepdims=True)\n",
    "\n",
    "    dH2 = np.dot(dA3,theta[\"w3\"].T)\n",
    "    dA2 = np.array(dH2, copy=True)\n",
    "    dA2[cache[\"A2\"] <= 0] = 0 # if the contribution is less than 0 deriv is 0 other wise 1 so 1*dh2\n",
    "\n",
    "    dw2 = (1/m)*(np.dot(cache[\"H1\"].T,dA2))\n",
    "    db2 = (1/m)*np.sum(dA2, axis=0, keepdims=True)\n",
    "\n",
    "    dH1 = np.dot(dA2,theta[\"w2\"].T)\n",
    "    dA1 = np.array(dH1, copy=True)\n",
    "    dA1[cache[\"A1\"] <= 0] = 0\n",
    "\n",
    "    dw1 = (1/m)*(np.dot(x.T,dA1))\n",
    "    db1 = (1/m)*np.sum(dA1, axis=0, keepdims=True)\n",
    "\n",
    "    grad = {\"dw1\": dw1, \"db1\": db1,\n",
    "    \"dw2\": dw2, \"db2\": db2,\n",
    "    \"dw3\": dw3, \"db3\": db3}\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f34fa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(theta, grad, neta = 0.01):\n",
    "    theta[\"w1\"] -= neta * grad[\"dw1\"]\n",
    "    theta[\"b1\"] -= neta * grad[\"db1\"]\n",
    "    \n",
    "    theta[\"w2\"] -= neta * grad[\"dw2\"]\n",
    "    theta[\"b2\"] -= neta * grad[\"db2\"]\n",
    "    \n",
    "    theta[\"w3\"] -= neta * grad[\"dw3\"]\n",
    "    theta[\"b3\"] -= neta * grad[\"db3\"]\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956900c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Loss_calc(y,H3): #calculate the cross entropy loss (as we are focusing on binary classification)\n",
    "    m = y.shape[0] #total samples\n",
    "    corr = 1e-8\n",
    "    H3 = np.clip(H3, corr, 1 - corr) # avoiding the log(0) and log(1) cases\n",
    "    \n",
    "    # weighted Cross entropy loss\n",
    "    loss = - (1/m)*np.sum(y * np.log(H3) + 3*(1 - y)*np.log(1 - H3))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0296959b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(ep):\n",
    "    # layer 1 \n",
    "    w1 = np.random.randn(108,65)*np.sqrt(2/108) #He-initialization of weights\n",
    "    b1 = np.zeros((1,65))\n",
    "    # layer 2\n",
    "    w2 = np.random.randn(65,33)*np.sqrt(2/65)\n",
    "    b2 = np.zeros((1,33)) \n",
    "    # layer 3\n",
    "    w3 = np.random.randn(33,1)*np.sqrt(2/33) \n",
    "    b3 = np.zeros((1,1))\n",
    "    # applying the forwardPass algorithm with backprop for learning the weights\n",
    "    theta = {\"w1\":w1, \"b1\":b1, \"w2\":w2, \"b2\":b2, \"w3\":w3, \"b3\":b3}\n",
    "    iter = 0\n",
    "    neta = 0.01\n",
    "    for epoch in range(ep): # outer loop for training different epochs, to stop the network from memorising the training data\n",
    "        ind = np.arange(x_train.shape[0])\n",
    "        np.random.shuffle(ind)\n",
    "        x_shf = x_train[ind]\n",
    "        y_shf = y_train[ind]\n",
    "        # inner loop for minibatch grad descent\n",
    "        for i in range(0,x_shf.shape[0],128):\n",
    "            x_b = x_shf[i:i+128]\n",
    "            y_b = y_shf[i:i+128]\n",
    "\n",
    "            cache = forward_pass(x_b,theta) #cache is the activations + pre-activations\n",
    "            grad = backpropagation(x_b, y_b, cache, theta)\n",
    "            theta = update(theta,grad,neta)\n",
    "            if iter%100 == 0:\n",
    "            # Calculate current loss on the current batch\n",
    "                current_loss = Loss_calc(y_b, cache['H3'])\n",
    "                print(f\"Iteration {iter} | Loss: {current_loss:.3f}\")\n",
    "            iter+=1\n",
    "        neta = neta*0.97\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3db189b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 | Loss: 41.447\n",
      "Iteration 100 | Loss: 1.558\n",
      "Iteration 200 | Loss: 1.382\n",
      "Iteration 300 | Loss: 1.272\n",
      "Iteration 400 | Loss: 1.206\n",
      "Iteration 500 | Loss: 1.152\n",
      "Iteration 600 | Loss: 1.105\n",
      "Iteration 700 | Loss: 1.091\n",
      "Iteration 800 | Loss: 1.055\n",
      "Iteration 900 | Loss: 1.036\n",
      "Iteration 1000 | Loss: 1.040\n",
      "Iteration 1100 | Loss: 1.029\n",
      "Iteration 1200 | Loss: 1.000\n",
      "Iteration 1300 | Loss: 1.003\n",
      "Iteration 1400 | Loss: 0.993\n",
      "Iteration 1500 | Loss: 0.951\n",
      "Iteration 1600 | Loss: 1.010\n",
      "Iteration 1700 | Loss: 0.969\n",
      "Iteration 1800 | Loss: 1.003\n",
      "Iteration 1900 | Loss: 0.977\n",
      "Iteration 2000 | Loss: 0.960\n",
      "Iteration 2100 | Loss: 0.972\n",
      "Iteration 2200 | Loss: 0.934\n",
      "Iteration 2300 | Loss: 0.966\n",
      "Iteration 2400 | Loss: 0.944\n",
      "Iteration 2500 | Loss: 0.951\n",
      "Iteration 2600 | Loss: 1.014\n",
      "Iteration 2700 | Loss: 0.966\n",
      "Iteration 2800 | Loss: 0.934\n",
      "Iteration 2900 | Loss: 1.020\n",
      "Iteration 3000 | Loss: 0.971\n",
      "Iteration 3100 | Loss: 0.961\n",
      "Iteration 3200 | Loss: 0.956\n",
      "Iteration 3300 | Loss: 0.960\n",
      "Iteration 3400 | Loss: 0.966\n",
      "Iteration 3500 | Loss: 0.971\n",
      "Iteration 3600 | Loss: 0.945\n",
      "Iteration 3700 | Loss: 0.926\n",
      "Iteration 3800 | Loss: 1.007\n",
      "Iteration 3900 | Loss: 0.972\n",
      "Iteration 4000 | Loss: 1.022\n",
      "Iteration 4100 | Loss: 0.967\n",
      "Iteration 4200 | Loss: 0.971\n",
      "Iteration 4300 | Loss: 0.969\n",
      "Iteration 4400 | Loss: 0.926\n",
      "Iteration 4500 | Loss: 0.967\n",
      "Iteration 4600 | Loss: 0.956\n",
      "Iteration 4700 | Loss: 0.928\n",
      "Iteration 4800 | Loss: 0.966\n",
      "Iteration 4900 | Loss: 0.987\n",
      "Iteration 5000 | Loss: 0.936\n",
      "Iteration 5100 | Loss: 0.966\n",
      "Iteration 5200 | Loss: 1.013\n",
      "Iteration 5300 | Loss: 0.988\n",
      "Iteration 5400 | Loss: 0.976\n",
      "Iteration 5500 | Loss: 0.990\n",
      "Iteration 5600 | Loss: 0.989\n",
      "Iteration 5700 | Loss: 0.952\n",
      "Iteration 5800 | Loss: 0.984\n",
      "Iteration 5900 | Loss: 0.966\n",
      "Iteration 6000 | Loss: 0.980\n",
      "Iteration 6100 | Loss: 0.936\n",
      "Iteration 6200 | Loss: 0.970\n",
      "Iteration 6300 | Loss: 0.965\n",
      "Iteration 6400 | Loss: 0.955\n",
      "Iteration 6500 | Loss: 0.924\n",
      "Iteration 6600 | Loss: 0.951\n",
      "Iteration 6700 | Loss: 0.984\n",
      "Iteration 6800 | Loss: 0.988\n",
      "Iteration 6900 | Loss: 0.970\n",
      "Iteration 7000 | Loss: 0.964\n",
      "Iteration 7100 | Loss: 0.946\n",
      "Iteration 7200 | Loss: 0.961\n",
      "Iteration 7300 | Loss: 0.969\n",
      "Iteration 7400 | Loss: 0.979\n",
      "Iteration 7500 | Loss: 0.985\n",
      "Iteration 7600 | Loss: 0.951\n",
      "Iteration 7700 | Loss: 0.922\n",
      "Iteration 7800 | Loss: 0.963\n",
      "Iteration 7900 | Loss: 0.951\n",
      "Iteration 8000 | Loss: 0.983\n",
      "Iteration 8100 | Loss: 0.989\n",
      "Iteration 8200 | Loss: 1.002\n",
      "Iteration 8300 | Loss: 0.970\n",
      "Iteration 8400 | Loss: 0.961\n",
      "Iteration 8500 | Loss: 1.011\n",
      "Iteration 8600 | Loss: 0.936\n",
      "Iteration 8700 | Loss: 0.941\n",
      "Iteration 8800 | Loss: 0.965\n",
      "Iteration 8900 | Loss: 0.993\n",
      "Iteration 9000 | Loss: 0.983\n",
      "Iteration 9100 | Loss: 0.932\n",
      "Iteration 9200 | Loss: 0.976\n",
      "Iteration 9300 | Loss: 0.994\n",
      "Iteration 9400 | Loss: 0.984\n",
      "Iteration 9500 | Loss: 0.965\n",
      "Iteration 9600 | Loss: 0.941\n",
      "Iteration 9700 | Loss: 0.998\n",
      "Iteration 9800 | Loss: 0.993\n",
      "Iteration 9900 | Loss: 0.951\n",
      "Iteration 10000 | Loss: 0.951\n",
      "Iteration 10100 | Loss: 0.985\n",
      "Iteration 10200 | Loss: 0.982\n",
      "Iteration 10300 | Loss: 0.936\n",
      "Iteration 10400 | Loss: 0.977\n",
      "Iteration 10500 | Loss: 0.971\n",
      "Iteration 10600 | Loss: 0.973\n",
      "Iteration 10700 | Loss: 0.956\n",
      "Iteration 10800 | Loss: 0.955\n",
      "Iteration 10900 | Loss: 0.945\n",
      "Iteration 11000 | Loss: 0.948\n",
      "Iteration 11100 | Loss: 0.957\n",
      "Iteration 11200 | Loss: 0.970\n",
      "Iteration 11300 | Loss: 0.974\n",
      "Iteration 11400 | Loss: 0.965\n",
      "Iteration 11500 | Loss: 0.957\n",
      "Iteration 11600 | Loss: 0.975\n",
      "Iteration 11700 | Loss: 0.947\n",
      "Iteration 11800 | Loss: 1.012\n",
      "Iteration 11900 | Loss: 0.957\n",
      "Iteration 12000 | Loss: 0.983\n",
      "Iteration 12100 | Loss: 0.983\n",
      "Iteration 12200 | Loss: 0.960\n",
      "Iteration 12300 | Loss: 0.998\n",
      "Iteration 12400 | Loss: 0.970\n",
      "Iteration 12500 | Loss: 0.970\n",
      "Iteration 12600 | Loss: 0.993\n",
      "Iteration 12700 | Loss: 0.966\n",
      "--- Model Performance Report ---\n",
      "Accuracy:  76.52%\n",
      "Precision: 1.0000 (How many predicted high-earners actually are)\n",
      "Recall:    0.0062 (How many actual high-earners were caught)\n",
      "F1-Score:  0.0124 (Balanced harmonic mean)\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "# main\n",
    "\n",
    "theta = SGD(50)\n",
    "def evaluate_metrics(x_test, y_test, theta):\n",
    "    # 1. Get predictions\n",
    "    cache = forward_pass(x_test, theta)\n",
    "    # Convert probabilities to binary 0 or 1\n",
    "    predictions = (cache[\"H3\"] > 0.5).astype(int)\n",
    "    \n",
    "    # 2. Basic Accuracy\n",
    "    accuracy = np.mean(predictions == y_test) * 100\n",
    "    tp = np.sum((predictions == 1) & (y_test == 1))\n",
    "    \n",
    "    tn = np.sum((predictions == 0) & (y_test == 0))\n",
    "    fp = np.sum((predictions == 1) & (y_test == 0))\n",
    "    fn = np.sum((predictions == 0) & (y_test == 1))\n",
    "    \n",
    "    precision = tp / (tp + fp + 1e-15) \n",
    "    recall = tp / (tp + fn + 1e-15)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall + 1e-15)\n",
    "    \n",
    "    # Print a clean report\n",
    "    print(\"--- Model Performance Report ---\")\n",
    "    print(f\"Accuracy:  {accuracy:.2f}%\")\n",
    "    print(f\"Precision: {precision:.4f} (How many predicted high-earners actually are)\")\n",
    "    print(f\"Recall:    {recall:.4f} (How many actual high-earners were caught)\")\n",
    "    print(f\"F1-Score:  {f1_score:.4f} (Balanced harmonic mean)\")\n",
    "    print(\"--------------------------------\")\n",
    "    \n",
    "    return {\"acc\": accuracy, \"p\": precision, \"r\": recall, \"f1\": f1_score}\n",
    "\n",
    "# Usage:\n",
    "results = evaluate_metrics(x_test, y_test, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6fbc274f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 | Loss: 1.418\n",
      "Iteration 100 | Loss: 1.039\n",
      "Iteration 200 | Loss: 0.986\n",
      "Iteration 300 | Loss: 0.873\n",
      "Iteration 400 | Loss: 0.835\n",
      "Iteration 500 | Loss: 0.858\n",
      "Iteration 600 | Loss: 0.852\n",
      "Iteration 700 | Loss: 0.706\n",
      "Iteration 800 | Loss: 0.781\n",
      "Iteration 900 | Loss: 0.759\n",
      "Iteration 1000 | Loss: 0.705\n",
      "Iteration 1100 | Loss: 0.771\n",
      "Iteration 1200 | Loss: 0.677\n",
      "Iteration 1300 | Loss: 0.652\n",
      "Iteration 1400 | Loss: 0.630\n",
      "Iteration 1500 | Loss: 0.699\n",
      "Iteration 1600 | Loss: 0.645\n",
      "Iteration 1700 | Loss: 0.733\n",
      "Iteration 1800 | Loss: 0.604\n",
      "Iteration 1900 | Loss: 0.714\n",
      "Iteration 2000 | Loss: 0.619\n",
      "Iteration 2100 | Loss: 0.666\n",
      "Iteration 2200 | Loss: 0.610\n",
      "Iteration 2300 | Loss: 0.599\n",
      "Iteration 2400 | Loss: 0.832\n",
      "Iteration 2500 | Loss: 0.662\n",
      "Iteration 2600 | Loss: 0.813\n",
      "Iteration 2700 | Loss: 0.600\n",
      "Iteration 2800 | Loss: 0.548\n",
      "Iteration 2900 | Loss: 0.783\n",
      "Iteration 3000 | Loss: 0.690\n",
      "Iteration 3100 | Loss: 0.703\n",
      "Iteration 3200 | Loss: 0.677\n",
      "Iteration 3300 | Loss: 0.716\n",
      "Iteration 3400 | Loss: 0.696\n",
      "Iteration 3500 | Loss: 0.477\n",
      "Iteration 3600 | Loss: 0.648\n",
      "Iteration 3700 | Loss: 0.623\n",
      "Iteration 3800 | Loss: 0.708\n",
      "Iteration 3900 | Loss: 0.780\n",
      "Iteration 4000 | Loss: 0.700\n",
      "Iteration 4100 | Loss: 0.752\n",
      "Iteration 4200 | Loss: 0.730\n",
      "Iteration 4300 | Loss: 0.681\n",
      "Iteration 4400 | Loss: 0.761\n",
      "Iteration 4500 | Loss: 0.837\n",
      "Iteration 4600 | Loss: 0.583\n",
      "Iteration 4700 | Loss: 0.603\n",
      "Iteration 4800 | Loss: 0.764\n",
      "Iteration 4900 | Loss: 0.666\n",
      "Iteration 5000 | Loss: 0.603\n",
      "Iteration 5100 | Loss: 0.718\n",
      "Iteration 5200 | Loss: 0.685\n",
      "Iteration 5300 | Loss: 0.572\n",
      "Iteration 5400 | Loss: 0.634\n",
      "Iteration 5500 | Loss: 0.531\n",
      "Iteration 5600 | Loss: 0.653\n",
      "Iteration 5700 | Loss: 0.723\n",
      "Iteration 5800 | Loss: 0.577\n",
      "Iteration 5900 | Loss: 0.679\n",
      "Iteration 6000 | Loss: 0.632\n",
      "Iteration 6100 | Loss: 0.510\n",
      "Iteration 6200 | Loss: 0.612\n",
      "Iteration 6300 | Loss: 0.518\n",
      "Iteration 6400 | Loss: 0.660\n",
      "Iteration 6500 | Loss: 0.678\n",
      "Iteration 6600 | Loss: 0.648\n",
      "Iteration 6700 | Loss: 0.880\n",
      "Iteration 6800 | Loss: 0.430\n",
      "Iteration 6900 | Loss: 0.736\n",
      "Iteration 7000 | Loss: 0.706\n",
      "Iteration 7100 | Loss: 0.536\n",
      "Iteration 7200 | Loss: 0.642\n",
      "Iteration 7300 | Loss: 0.582\n",
      "Iteration 7400 | Loss: 0.634\n",
      "Iteration 7500 | Loss: 0.570\n",
      "Iteration 7600 | Loss: 0.799\n",
      "Iteration 7700 | Loss: 0.737\n",
      "Iteration 7800 | Loss: 0.661\n",
      "Iteration 7900 | Loss: 0.624\n",
      "Iteration 8000 | Loss: 0.662\n",
      "Iteration 8100 | Loss: 0.594\n",
      "Iteration 8200 | Loss: 0.760\n",
      "Iteration 8300 | Loss: 0.509\n",
      "Iteration 8400 | Loss: 0.459\n",
      "Iteration 8500 | Loss: 0.660\n",
      "Iteration 8600 | Loss: 0.512\n",
      "Iteration 8700 | Loss: 0.557\n",
      "Iteration 8800 | Loss: 0.517\n",
      "Iteration 8900 | Loss: 0.571\n",
      "Iteration 9000 | Loss: 0.787\n",
      "Iteration 9100 | Loss: 0.683\n",
      "Iteration 9200 | Loss: 0.538\n",
      "Iteration 9300 | Loss: 0.671\n",
      "Iteration 9400 | Loss: 0.561\n",
      "Iteration 9500 | Loss: 0.425\n",
      "Iteration 9600 | Loss: 0.592\n",
      "Iteration 9700 | Loss: 0.610\n",
      "Iteration 9800 | Loss: 0.794\n",
      "Iteration 9900 | Loss: 0.607\n",
      "Iteration 10000 | Loss: 0.724\n",
      "Iteration 10100 | Loss: 0.575\n",
      "Iteration 10200 | Loss: 0.506\n",
      "Iteration 10300 | Loss: 0.555\n",
      "Iteration 10400 | Loss: 0.720\n",
      "Iteration 10500 | Loss: 0.696\n",
      "Iteration 10600 | Loss: 0.674\n",
      "Iteration 10700 | Loss: 0.688\n",
      "Iteration 10800 | Loss: 0.672\n",
      "Iteration 10900 | Loss: 0.676\n",
      "Iteration 11000 | Loss: 0.689\n",
      "Iteration 11100 | Loss: 0.725\n",
      "Iteration 11200 | Loss: 0.783\n",
      "Iteration 11300 | Loss: 0.491\n",
      "Iteration 11400 | Loss: 0.648\n",
      "Iteration 11500 | Loss: 0.642\n",
      "Iteration 11600 | Loss: 0.826\n",
      "Iteration 11700 | Loss: 0.553\n",
      "Iteration 11800 | Loss: 0.678\n",
      "Iteration 11900 | Loss: 0.710\n",
      "Iteration 12000 | Loss: 0.635\n",
      "Iteration 12100 | Loss: 0.616\n",
      "Iteration 12200 | Loss: 0.695\n",
      "Iteration 12300 | Loss: 0.614\n",
      "Iteration 12400 | Loss: 0.549\n",
      "Iteration 12500 | Loss: 0.615\n",
      "Iteration 12600 | Loss: 0.877\n",
      "Iteration 12700 | Loss: 0.643\n",
      "--- Results with Min-Max Scaling ---\n",
      "--- Model Performance Report ---\n",
      "Accuracy:  84.19%\n",
      "Precision: 0.7102 (How many predicted high-earners actually are)\n",
      "Recall:    0.5588 (How many actual high-earners were caught)\n",
      "F1-Score:  0.6254 (Balanced harmonic mean)\n",
      "--------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': 84.1901603095633,\n",
       " 'p': 0.7101784534038335,\n",
       " 'r': 0.5587623504940198,\n",
       " 'f1': 0.6254365541327118}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def min_max_scaling(df):\n",
    "    # Identify numerical columns (exclude one-hot encoded ones)\n",
    "    num_cols = [\"age\", \"fnlwgt\", \"education-num\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"]\n",
    "    \n",
    "    # Apply Min-Max formula: (x - min) / (max - min) \n",
    "    for col in num_cols:\n",
    "        col_min = df[col].min()\n",
    "        col_max = df[col].max()\n",
    "        df[col] = (df[col] - col_min) / (col_max - col_min)\n",
    "    return df\n",
    "\n",
    "x_t_r_scaled = min_max_scaling(x_t_r.copy())\n",
    "x_te_r_scaled = min_max_scaling(x_te_r.copy())\n",
    "\n",
    "x_train_scaled = pd.get_dummies(x_t_r_scaled)\n",
    "x_test_scaled = pd.get_dummies(x_te_r_scaled)\n",
    "x_train_scaled, x_test_scaled = x_train_scaled.align(x_test_scaled, join='left', axis=1, fill_value=0)\n",
    "\n",
    "x_train = x_train_scaled.values.astype(np.float32)\n",
    "x_test = x_test_scaled.values.astype(np.float32)\n",
    "theta_scaled = SGD(50)\n",
    "\n",
    "print(\"--- Results with Min-Max Scaling ---\")\n",
    "evaluate_metrics(x_test, y_test, theta_scaled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
